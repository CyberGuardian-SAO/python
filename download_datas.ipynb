      "source": [
        "#@title 默认标题文本\n",
        "#python抓取两步路数据\n",
        "#第一步安装必要库和接口\n",
        "#修正时间，因为Google colab服务器在美国所以输出的时间会相差8小时\n",
        "!date -R\n",
        "!apt-get install ntpdate\n",
        "!ntpdate ntp.sjtu.edu.cn\n",
        "!date -R\n",
        "import os\n",
        "os.environ['TZ'] = \"Asia/Shanghai\"\n",
        "# install chromium, its driver, and selenium\n",
        "!pip install selenium\n",
        "!pip install requests-html\n",
        "!pip install beautifulsoup4\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install xmltodict\n",
        "#导入必要接口\n",
        "import urllib.request\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import xlwt\n",
        "import bs4\n",
        "import re\n",
        "from datetime import datetime   \n",
        "from selenium import webdriver\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "#（1）加上请求头，伪装成浏览器访问下面使request加头文件的方法,如果用chromedrier则不能这样用，而要用在option.add_argument里面\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
        "          'Cookie':'UM_distinctid=17047f9723c1-00ad6085cfe524-313f69-15c000-17047f9724470; JSESSIONID=797A1484051DF3D929A87FFE82AD7989-n2; CNZZDATA1000341086=438850946-1581752503-null%7C1584245957'}\n",
        "# set options to be headless, ..\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results\n",
        "#（测试是否可以得到源码）\n",
        "wd = webdriver.Chrome('chromedriver',options=options)\n",
        "wd.get(\"http://www.2bulu.com/track/list-%E9%9F%AD%E8%8F%9C%E5%B2%AD-----1.htm?sortType=2\")#输入测试网址\n",
        "#print(wd.page_source) 测试的时候就可以打开\n",
        "#上一部分和下一部分最好分成两部分执行，这样第一部分就不需要重新执行了"
      ],
      "source": [
        "#第二部开始操作\n",
        "#（1）用这条命令知道当前在抓取哪个链接，如果发生错误便于调试\n",
        "def start_requests(url):      #这里的url只是传参，比如你使用这个函数的时候start_requests(gx)，它会把原来url的地方替换成gx\n",
        "    print(url)          #用这个接口打印当前在抓取哪个链接，如果发生错误便于调试\n",
        "    r = requests.get(url)   #这个接口的意思是实现访问url指代的网址\n",
        "    return r.content      #这个函数的作用是返回r的内容即requests.get得到的\n",
        "\n",
        "#（2）使用chromedriver模拟打开网页获取目标网址的源代码\n",
        "def get_page_sourse(url):\n",
        "  options = webdriver.ChromeOptions()       #定义options\n",
        "  options.add_argument('--headless')         #浏览器不提供可视化页面.linux下如果系统不支持可视化不加\n",
        "  options.add_argument('--no-sandbox')        #不启动沙盒，Bypass OS security model\n",
        "  options.add_argument('--disable-dev-shm-usage')   #overcome limited resource problems\n",
        "  options.add_argument('--disable-gpu') #谷歌文档提到需要加上这个属性来规避bug\n",
        "  options.add_argument('--hide-scrollbars') #隐藏滚动条, 应对一些特殊页面\n",
        "  #options.add_argument('blink-settings=imagesEnabled=false') #不加载图片, 提升速度，不爬图片的时候可以打开\n",
        "  #options.add_argument('--proxy-server=http://222.95.240.228:3000') #设置代理\n",
        "  options.add_argument('User-Agent:\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\"')#chromedriver添加头的做法\n",
        "  driver = webdriver.Chrome('chromedriver', options=options) #后面3步是为了得到page_sourese即页面的源码，如果正常request取不到的话，一般用于动态加载的\n",
        "  \n",
        "  driver.get(url)\n",
        "  page_sourse=driver.page_source\n",
        "  driver.close()                  #用完driver记得关闭，节省内存空间\n",
        "  return page_sourse\n",
        "\n",
        "#（3）解析一级源代码，找到二级地址并实现拼接\n",
        "def parse_music(text):\n",
        "  page_sourse = get_page_sourse(url)       #调用上一个函数即get_page_sourse(url)来得到一级界面源码\n",
        "  soup = BeautifulSoup(page_sourse,\"lxml\")    #通过beautifulsoup中lxml解码器解析源码\n",
        "  #jake=soup.find_all('a',target='_blank'，src='images') #找a标签中，同时有target与src属性的\n",
        "  jakes=soup.find_all('img',onerror=\"javascript:this.src='/images/defaultTrack.png';\") #通过find_all找到soup中所有img标签且这个标签中含有onerror属性\n",
        "  html_list=[]                   #设置一个数组或者说集合来装符合要求的二级连接\n",
        "  #idx=0\n",
        "  for jake in jakes:              #打印含有img的父标签集合\n",
        "    if jake.parent !=None:          #加判断，把所有img中有父节点的匹配出来\n",
        "        html_list.append(jake.parent)    #通过加.append把收集好的数据存到html_list这个集合中\n",
        "        #idx=idx+1\n",
        "        #print(idx)检测是否逐一筛选出img页面\n",
        "  lst2 = list(set(html_list))          #去重，如果html_list中有重复的连接，这一步就可以去掉\n",
        "  #print(lst2)检测是否去重成功\n",
        "  #all_music = lst2.find_all('a', {'href': re.compile('/track/.*2$')}) #找list2中符合条件的href正则法\n",
        "  \n",
        "  pages = []                  #设置pages集合来装完整的一级目录集合，因为上一步搜集到的是相对地址需要通过这一个\n",
        "  for link in lst2:\n",
        "    current_url = \"http://www.2bulu.com/\"  #如果是相对地址就拼接\n",
        "    relative_url=link['href']         #找到单个link中href的属性内容\n",
        "    #print(relative_url)           #确认是否找到\n",
        "    complete_url = urljoin(current_url, relative_url)  #urljoin接口实现拼接，即把第1个参数与第2个参数拼接在一起\n",
        "    pages.append(complete_url)        #把拼接好的完整链接用append累加存在pages这个集合内\n",
        "  return pages                \n",
        "#（4）解析二级目录源代码，并提取关键词和标签\n",
        "def parse_page(text):\n",
        "  #url=\"http://www.2bulu.com/track/t-ockmzMSsUKzp%252FR2KBg5Tzw%253D%253D.htm?tabType=2xia\"  #可以使用这个函数实现单独测试网页是否需要动态加载\n",
        "  #print(page_sourse)\n",
        "  soup = BeautifulSoup(text, 'lxml')    #使用BeautifulSoup解析text传过来的网站，text是形参，第2个参数lxml是BeautifulSoup3个解析工具之一，它的速度最快\n",
        "  #print(soup)查看二级页面是否也是动态加载，如果是则需要改用chrome\n",
        "  lis = soup.find(\"ul\", id =\"track_marker_div\",class_=\"biaozhu_list\")  #查找ul标签中，属性id=track_marker_div,属性class=biaozhu_list的整个ul\n",
        "  #print(lis)                 #打印下确认是否正确抓取\n",
        "  datas=[]                  #定义datas的数组装二级目录中想要抓取的标签数据\n",
        "  for li in lis:              #循环找lis中即ul标签中的每一个li子标签\n",
        "    time.sleep(1)              #找到一个下载完毕后就睡1s\n",
        "    #print(li)               #确认是否成功找到\n",
        "    times=li.find('p',class_='date').text  #找时间标签，find是只找第一个p标签，属性为date的,后面的.text就是提取此标签中的文本内容\n",
        "    if re.findall(r'\\d{1,4}-\\d{1,2}-\\d{1,2} \\d{1,2}:\\d{1,2}', times):         #当找到这样的结构2019-12-28 08:02就进行下面的切割{1，4}对应每个位置有多少数字 \n",
        "      result_year = time.strftime(\"%Y\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\")) #取年份，当年月日时都匹配正确后，改变输出，这是只输出年份\n",
        "      result_month = time.strftime(\"%m\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\")) #取月份   \n",
        "      result_day = time.strftime(\"%d\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\")) #strptime作用是将第1个参数的格式与第2个参数匹配\n",
        "      result_hour = time.strftime(\"%H\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\"))#strftime作用将第2个参数输出成第1个参数的形式\n",
        "    #names=li.find('p',class_='text').text\n",
        "    name=li.find('p',{\"class\": re.compile('[^\\d,\\.]+')}).string #通过find提取p标签中，过滤数字和点.的文本标注内容，^\\d的意思是除开数字，\\.就是除了.因为是特殊符号所有要加反斜杠 \n",
        "    names=re.findall('[^\\s,\\d,\\.:,\\-]+',name)          #实现匹配除空格、数字、标点(.)的字符\n",
        "    link=li.find('img',class_='lazy_img')            #这个标签内存了图片的下载链接\n",
        "    if (link is not None):                  #加个判断，防止没找到的时候报错，只有在找到才进行下面的提取下载图片工作\n",
        "      links = link[\"data-original\"]             #用links来装下载链接\n",
        "      id=li.find('div',class_='point').get('id')        #查找每张图片的id，通过get可以得到id=\"225449118\"它属性的值，因为不是文本所以用get\n",
        "      id=int(id)                        #id是属性值，所以要强转为int型 \n",
        "      datas.clear()                      #append会累加每次的数据，所以用clear清空datas中的数据，这样datas中每次只有新图片的所有信息\n",
        "      datas.append({\"n\":names,\"年\":result_year,\"月\":result_month,\"日\":result_day,\"时\":result_hour,\"id\":id}) #数据写进datas中\n",
        "    #下载照片\n",
        "      response = requests.get(links)             #通过requests.get实现下载照片\n",
        "      fb = open('/content/drive/My Drive/骑田岭/图片/%s.jpg'%str(id),'wb')  #创建文件以wb代表以二进制方式写入，图片一般都是用二进制编码后写入，%s对应的是后面的%str(id)即以id名命名图片\n",
        "      fb.write(response.content)                #写入request访问链接得到的内容\n",
        "      fb.close()                        #关闭，减少内存占用\n",
        "    #下载标签数据\n",
        "      with open(\"/content/drive/My Drive/骑田岭/json/%s.json\"%str(id), \"w\") as fout: #创建文件把标签属性信息写入并以json格式保存\n",
        "        for data in datas:                              #提取datas中的每个标签，其实如果有上面的clear一步就可以不这样写，多余了\n",
        "          fout.write(json.dumps(data, ensure_ascii=False)+\"\\n\")           #json.dumps实现写入\n",
        "    else:                            #如果没有找到图片链接就跳过\n",
        "      pass\n",
        "    #以src命名图片和json文件\n",
        "  return datas \n",
        "\n",
        "for idx in range(0,23):                     #for循环，设置idx参数，range实现是闭区间，即range(0,24)，则idx从0开始24结束，idx=0...\n",
        "  #result_list = []\n",
        "  time.sleep(1)                          #二级页面抓取完后睡1秒\n",
        "  #九嶷山http://www.2bulu.com/track/list-%E4%B9%9D%E5%B6%B7%E5%B1%B1-----23.htm?sortType=2\n",
        "  #骑田岭http://www.2bulu.com/track/list-%E9%AA%91%E7%94%B0%E5%B2%AD-----1.htm?sortType=2\n",
        "  url = f\"http://www.2bulu.com/track/list-%E9%AA%91%E7%94%B0%E5%B2%AD-----{idx+1}.htm?sortType=2\" #通过f,{idx+1},实现翻页抓取\n",
        "  print(\"spider当前正在爬取第 %s 页的轨迹\"%(idx+1))\n",
        "  text = start_requests(url)                  #调用start_requests(url)打印正在爬取的链接\n",
        "  time.sleep(1)\n",
        "  pageurls = parse_music(text)                 #清理好的只有图片的二级链接集合\n",
        "  print(\"当前页面含有图片标记的轨迹数为 %s 条\"%len(pageurls))\n",
        "  flag=1                             #设置个参数标记正在打印的链接\n",
        "  for pageurl in pageurls:                  #提取含有二级链接集合中的每一个链接\n",
        "    print(\"正在爬取第 %s 条链接 开始时间： %s\"%(flag,time.ctime())) #打印时间\n",
        "    flag=flag+1\n",
        "    page = start_requests(pageurl)             #打印正在分析的链接\n",
        "    jake = get_page_sourse(pageurl)            #用chrome模拟打开二级页面得到网页源码并存入jake\n",
        "    parse_page(jake)                     #分析源码实现下载保存\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
