{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "download datas",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16hfm29WhiZfk0ot98ygCWRP7GOU3dsFS",
      "authorship_tag": "ABX9TyO2/ZAT5yXKMmFrUVgVsx0I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CyberGuardian-SAO/python/blob/master/download_datas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j--CNrIRi-rA",
        "outputId": "c3977f28-a2fd-4a40-811f-344bf58e13df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "# python 3.7\n",
        "#测试是否可以实现翻页的效果生成页面\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
        "           'Cookie':'UM_distinctid=17047f9723c1-00ad6085cfe524-313f69-15c000-17047f9724470; JSESSIONID=797A1484051DF3D929A87FFE82AD7989-n2; CNZZDATA1000341086=438850946-1581752503-null%7C1584245957'}\n",
        "def download_all_htmls():\n",
        "    \"\"\"\n",
        "    下载所有列表页面的HTML，用于后续的分析\n",
        "    \"\"\"\n",
        "    htmls = []\n",
        "    for idx in range(3):\n",
        "      #韭菜岭http://www.2bulu.com/track/list-%E9%9F%AD%E8%8F%9C%E5%B2%AD-----1.htm?sortType=2\n",
        "      #舜皇山http://www.2bulu.com/track/list-%E8%88%9C%E7%9A%87%E5%B1%B1-----1.htm?sortType=2\n",
        "        url = f\"http://www.2bulu.com/track/list-%E9%9F%AD%E8%8F%9C%E5%B2%AD-----{idx+1}.htm?sortType=2\"\n",
        "        print(url)\n",
        "        r = requests.get(url)\n",
        "        if r.status_code != 200:\n",
        "            raise Exception(\"error\")\n",
        "        htmls.append(r.text)\n",
        "    return htmls\n",
        "htmls = download_all_htmls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://www.2bulu.com/track/list-%E9%9F%AD%E8%8F%9C%E5%B2%AD-----1.htm?sortType=2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fd38c13306cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mhtmls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhtmls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhtmls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_all_htmls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-fd38c13306cf>\u001b[0m in \u001b[0;36mdownload_all_htmls\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"http://www.2bulu.com/track/list-%E9%9F%AD%E8%8F%9C%E5%B2%AD-----{idx+1}.htm?sortType=2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVwtFjOllAsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "# python 3.7\n",
        "#简单爬虫练习一\n",
        "#引入系统类库\n",
        "import sys\n",
        "# 使用文档解析类库\n",
        "from bs4 import BeautifulSoup\n",
        "# 使用网络请求类库\n",
        "import urllib.request\n",
        "# 输入网址\n",
        "html_doc = \"https://www.baidu.com\"\n",
        "if len(sys.argv)>1:\n",
        "   website=sys.argv[1]\n",
        "   if(website is not None):\n",
        "        html_doc= sys.argv[1]\n",
        "# 获取请求\n",
        "req = urllib.request.Request(html_doc)\n",
        "# 打开页面\n",
        "webpage = urllib.request.urlopen(req)\n",
        "# 读取页面内容\n",
        "html = webpage.read()\n",
        "# 解析成文档对象\n",
        "soup = BeautifulSoup(html, 'html.parser')   #文档对象\n",
        "# 非法URL 1\n",
        "invalidLink1='#'\n",
        "# 非法URL 2\n",
        "invalidLink2='javascript:void(0)'\n",
        "# 集合\n",
        "result=set()\n",
        "# 计数器\n",
        "mycount=0\n",
        "#查找文档中所有a标签\n",
        "for k in soup.find_all('a'):\n",
        "    #print(k)\n",
        "    #查找href标签\n",
        "    link=k.get('href')\n",
        "    # 过滤没找到的\n",
        "    if(link is not None):\n",
        "          #过滤非法链接\n",
        "          if link==invalidLink1:\n",
        "            pass\n",
        "          elif link==invalidLink2:\n",
        "            pass\n",
        "          elif link.find(\"javascript:\")!=-1:\n",
        "            pass\n",
        "          else:\n",
        "            mycount=mycount+1\n",
        "            #print(mycount,link)\n",
        "            result.add(link)\n",
        "#print(\"打印超链接个数:\",mycount)\n",
        "#print(\"打印超链接列表\",result)\n",
        "f = open(r'result.txt','w',encoding='utf-8')  #文件路径、操作模式、编码  # r''\n",
        "for a in result:\n",
        "    f.write(a+\"\\n\")\n",
        "f.close()\n",
        "print(\"\\r\\n扫描结果已写入到result.txt文件中\\r\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RomMtwdd7d7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "# python 3.7\n",
        "#简单爬虫练习二\n",
        "#引入系统类库\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import time\n",
        "import xlwt\n",
        "import urllib.request\n",
        "from urllib.request import URLError\n",
        " \n",
        "# 调用chrome浏览器并后台运行\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
        "\n",
        "driver.get(\"http://www.2bulu.com/track/list-%E9%9F%AD%E8%8F%9C%E5%B2%AD-----5.htm?sortType=2\")   # 要测试的页面\n",
        "\n",
        "\n",
        "#打开网页\n",
        "\n",
        "#time.sleep(5)\n",
        " \n",
        "#获取网页信息\n",
        "html=driver.page_source\n",
        "soup=BeautifulSoup(html,'lxml')\n",
        " \n",
        "#用soup来获得所有'tr'标签\n",
        "list=soup.find_all(name='div',attrs = {'class':'guiji_discription'})\n",
        "result=[]\n",
        " \n",
        "#将所有符合规则的'tr'标签里面的内容提取出来\n",
        "for each in list:\n",
        "    key = each.find('span',{'class':'s7'})\n",
        "    point = each.find('div',{'class':'right_pic'})\n",
        "    if point !=None:\n",
        "        point = point.find('span')\n",
        "    if rank!=None and key!=None and point!=None :\n",
        "        result.append([key.string,point.string])\n",
        " \n",
        "#新建xls对象\n",
        "workbook = xlwt.Workbook(encoding = 'utf-8')\n",
        "worksheet = workbook.add_sheet('Baidu Rank Data')\n",
        "worksheet.write(0,1, label = 'key')\n",
        "worksheet.write(0,2, label = 'point')\n",
        " \n",
        "#设置列宽\n",
        "col = worksheet.col(1)\n",
        "col.width=5000\n",
        " \n",
        "#写入数据\n",
        "i=1\n",
        "for each in result:\n",
        "    rank=str(each[0])\n",
        "    key=str(each[1])\n",
        "    point=str(each[2])\n",
        "    worksheet.write(i,0,rank)\n",
        "    worksheet.write(i,1,key)\n",
        "    worksheet.write(i,2,point)\n",
        "    i+=1\n",
        " \n",
        "#保存\n",
        "workbook.save(r'C:\\Users\\me\\Desktop\\Data.xls')\n",
        " \n",
        "print(result)\n",
        "#print(len(result))\n",
        "#print(len(list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAf53bME3143",
        "colab_type": "code",
        "outputId": "f28f61e1-9262-4c9c-c00a-46a72085b2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#抓取二级页面目录的图片和标签数据\n",
        "def parse_music():\n",
        "  url=\"http://www.2bulu.com/track/t-2hzcA6K5wYvp%252FR2KBg5Tzw%253D%253D.htm?tabType=2\"\n",
        "  driver = webdriver.Chrome('chromedriver', options=options)\n",
        "  driver.get(url)\n",
        "  page_sourse=driver.page_source\n",
        "  #print(page_sourse)\n",
        "  soup = BeautifulSoup(page_sourse, 'lxml')\n",
        "  #print(soup)\n",
        "  lis = soup.find(\"ul\", id =\"track_marker_div\")\n",
        "  print(lis)\n",
        "  datas=[]\n",
        "  for li in lis: \n",
        "    times=li.find('p',class_='date').text\n",
        "    if re.findall(r'\\d{1,4}-\\d{1,2}-\\d{1,2} \\d{1,2}:\\d{1,2}', times):\n",
        "      result_year = time.strftime(\"%Y\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\"))\n",
        "      result_month = time.strftime(\"%m\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\"))   \n",
        "      result_day = time.strftime(\"%d\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\")) #strptime作用是将第1个参数的格式与第2个参数匹配\n",
        "      result_hour = time.strftime(\"%H\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\"))#strftime作用将第2个参数输出成第1个参数的形式\n",
        "    #names=li.find('p',class_='text').text\n",
        "    name=li.find('p',{\"class\": re.compile('[^\\d,\\.]+')}).string\n",
        "    names=re.findall('[^\\s,\\d,\\.:,\\-]+',name)#实现匹配除空格、数字、标点(.)的字符\n",
        "    link=li.find('img',class_='lazy_img')\n",
        "    if (link is not None): \n",
        "      links = link[\"data-original\"]\n",
        "      id=li.find('div',class_='point').get('id')\n",
        "      id=int(id)#id是属性值，所以要强转为int型\n",
        "      datas.clear()#清空datas中的数据\n",
        "      datas.append({\"n\":names,\"年\":result_year,\"月\":result_month,\"日\":result_day,\"时\":result_hour,\"id\":id}) \n",
        "    #下载照片\n",
        "      response = requests.get(links) #获取请求\n",
        "    #name = '233'  #图片名称\n",
        "      fb = open('/content/图片/%s.jpg'%str(id),'wb')  #wb代表以二进制方式写入\n",
        "      fb.write(response.content)  #写入\n",
        "      fb.close()  #关闭\n",
        "    #下载标签数据\n",
        "      with open(\"/content/json/%s.json\"%str(id), \"w\") as fout:\n",
        "        for data in datas:\n",
        "          fout.write(json.dumps(data, ensure_ascii=False)+\"\\n\")\n",
        "    else:\n",
        "      pass\n",
        "    #以src命名图片和json文件\n",
        "  return datas  \n",
        "\n",
        "parse_music()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ul class=\"biaozhu_list\" id=\"track_marker_div\"><li><div class=\"point\" id=\"203666343\" marker_id=\"203666343\"><div class=\"info\"><div class=\"info_show on\"><img alt=\"\" src=\"/images/track/biaozhu.png\"/><a href=\"#\">0.04km</a></div><div class=\"biaozhu_pic\"><a class=\"group_markImg\" href=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5EKBLYt1Quxlw%3D%3D%0A\" markerindex=\"1\" rel=\"group_markImg\"><img class=\"lazy_img\" data-original=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5HWiY55Llxrug%3D%3D%0A\" src=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5HWiY55Llxrug%3D%3D%0A\" style=\"display: inline;\"/><span class=\"point_img_ok\"></span></a><p class=\"date\">2019-12-06 08:52:38</p></div></div></div></li><li><div class=\"point\" id=\"203666344\" marker_id=\"203666344\"><div class=\"info\"><div class=\"info_show on\"><img alt=\"\" src=\"/images/track/biaozhu.png\"/><a href=\"#\">0.19km</a></div><div class=\"biaozhu_pic\"><a class=\"group_markImg\" href=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5EwlEME86zMoQ%3D%3D%0A\" markerindex=\"2\" rel=\"group_markImg\"><img class=\"lazy_img\" data-original=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5FIGvigxHrhLA%3D%3D%0A\" src=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5FIGvigxHrhLA%3D%3D%0A\" style=\"display: inline;\"/><span class=\"point_img_ok\"></span></a><p class=\"date\">2019-12-06 08:55:02</p></div></div></div></li><li><div class=\"point\" id=\"203666345\" marker_id=\"203666345\"><div class=\"info\"><div class=\"info_show on\"><img alt=\"\" src=\"/images/track/biaozhu.png\"/><a href=\"#\">0.45km</a></div><div class=\"biaozhu_pic\"><a class=\"group_markImg\" href=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5Eef9MgRK08oQ%3D%3D%0A\" markerindex=\"3\" rel=\"group_markImg\"><img class=\"lazy_img\" data-original=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5EMauGYU0u%2Biw%3D%3D%0A\" src=\"https://down-files.2bulu.com/f/d1?downParams=E6rpf59vd5EMauGYU0u%2Biw%3D%3D%0A\" style=\"display: inline;\"/><span class=\"point_img_ok\"></span></a><p class=\"date\">2019-12-06 09:02:48</p></div></div></div></li></ul>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 203666345, 'n': [], '年': '2019', '日': '06', '时': '09', '月': '12'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP4wdBgNwHbm",
        "colab_type": "code",
        "outputId": "b12dc20a-42f7-415e-d4c9-e68cc67ac2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#json转xml\n",
        "!pip install xmltodict\n",
        "import xmltodict\n",
        "import json\n",
        "import os\n",
        "\n",
        "# json to xml\n",
        "def jsonToXml(json_str):\n",
        "    try:\n",
        "        xml_str=\"\"\n",
        "        xml_str = xmltodict.unparse(json_str, encoding='utf-8')\n",
        "    except:\n",
        "        xml_str = xmltodict.unparse({'request': json_str}, encoding='utf-8')\n",
        "    finally:\n",
        "        return xml_str\n",
        "\n",
        "def json_to_xml(json_path,xml_path):\n",
        "    if(os.path.exists(xml_path)==False):\n",
        "        os.makedirs(xml_path)\n",
        "    dir = os.listdir(json_path)\n",
        "    for file in dir:\n",
        "        file_list=file.split(\".json\")#以.json为单位进行切割\n",
        "        with open(os.path.join(json_path,file), 'r') as load_f:\n",
        "            load_dict = json.load(load_f)\n",
        "        json_result = jsonToXml(load_dict)\n",
        "        f = open(os.path.join(xml_path,file_list[0]+\".xml\"), 'w', encoding=\"UTF-8\")\n",
        "        f.write(json_result)\n",
        "        f.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    json_path=r\"/content/drive/My Drive/骑田岭/json\"  #该目录为存放json文件的路径  ps:目录中只能存放json文件\n",
        "    xml_path=r\"/content/drive/My Drive/骑田岭/xml\"   #该目录为放xml文件的路径\n",
        "    json_to_xml(json_path,xml_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (0.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWPt2H98261Y",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title 默认标题文本\n",
        "#python抓取两步路数据\n",
        "#第一步安装必要库和接口\n",
        "#修正时间，因为Google colab服务器在美国所以输出的时间会相差8小时\n",
        "!date -R\n",
        "!apt-get install ntpdate\n",
        "!ntpdate ntp.sjtu.edu.cn\n",
        "!date -R\n",
        "import os\n",
        "os.environ['TZ'] = \"Asia/Shanghai\"\n",
        "# install chromium, its driver, and selenium\n",
        "!pip install selenium\n",
        "!pip install requests-html\n",
        "!pip install beautifulsoup4\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install xmltodict\n",
        "#导入必要接口\n",
        "import urllib.request\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import xlwt\n",
        "import bs4\n",
        "import re\n",
        "from datetime import datetime   \n",
        "from selenium import webdriver\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "#（1）加上请求头，伪装成浏览器访问下面使request加头文件的方法,如果用chromedrier则不能这样用，而要用在option.add_argument里面\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',\n",
        "          'Cookie':'UM_distinctid=17047f9723c1-00ad6085cfe524-313f69-15c000-17047f9724470; JSESSIONID=797A1484051DF3D929A87FFE82AD7989-n2; CNZZDATA1000341086=438850946-1581752503-null%7C1584245957'}\n",
        "# set options to be headless, ..\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results\n",
        "#（测试是否可以得到源码）\n",
        "wd = webdriver.Chrome('chromedriver',options=options)\n",
        "wd.get(\"http://www.2bulu.com/track/list-%E9%9F%AD%E8%8F%9C%E5%B2%AD-----1.htm?sortType=2\")#输入测试网址\n",
        "#print(wd.page_source) 测试的时候就可以打开\n",
        "#上一部分和下一部分最好分成两部分执行，这样第一部分就不需要重新执行了"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnnN3mP3wjjT",
        "colab_type": "code",
        "outputId": "05779da8-6a0f-4f1a-c665-7e3f65461a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#移除图片、json、xml中所有的内容方便测试\n",
        "!rm /content/图片/*\n",
        "!rm /content/json/*\n",
        "!rm /content/xml/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/xml/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHw7oNt2oSKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#第二部开始操作\n",
        "#（1）用这条命令知道当前在抓取哪个链接，如果发生错误便于调试\n",
        "def start_requests(url):      #这里的url只是传参，比如你使用这个函数的时候start_requests(gx)，它会把原来url的地方替换成gx\n",
        "    print(url)          #用这个接口打印当前在抓取哪个链接，如果发生错误便于调试\n",
        "    r = requests.get(url)   #这个接口的意思是实现访问url指代的网址\n",
        "    return r.content      #这个函数的作用是返回r的内容即requests.get得到的\n",
        "\n",
        "#（2）使用chromedriver模拟打开网页获取目标网址的源代码\n",
        "def get_page_sourse(url):\n",
        "  options = webdriver.ChromeOptions()       #定义options\n",
        "  options.add_argument('--headless')         #浏览器不提供可视化页面.linux下如果系统不支持可视化不加\n",
        "  options.add_argument('--no-sandbox')        #不启动沙盒，Bypass OS security model\n",
        "  options.add_argument('--disable-dev-shm-usage')   #overcome limited resource problems\n",
        "  options.add_argument('--disable-gpu') #谷歌文档提到需要加上这个属性来规避bug\n",
        "  options.add_argument('--hide-scrollbars') #隐藏滚动条, 应对一些特殊页面\n",
        "  #options.add_argument('blink-settings=imagesEnabled=false') #不加载图片, 提升速度，不爬图片的时候可以打开\n",
        "  #options.add_argument('--proxy-server=http://222.95.240.228:3000') #设置代理\n",
        "  options.add_argument('User-Agent:\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\"')#chromedriver添加头的做法\n",
        "  driver = webdriver.Chrome('chromedriver', options=options) #后面3步是为了得到page_sourese即页面的源码，如果正常request取不到的话，一般用于动态加载的\n",
        "  \n",
        "  driver.get(url)\n",
        "  page_sourse=driver.page_source\n",
        "  driver.close()                  #用完driver记得关闭，节省内存空间\n",
        "  return page_sourse\n",
        "\n",
        "#（3）解析一级源代码，找到二级地址并实现拼接\n",
        "def parse_music(text):\n",
        "  page_sourse = get_page_sourse(url)       #调用上一个函数即get_page_sourse(url)来得到一级界面源码\n",
        "  soup = BeautifulSoup(page_sourse,\"lxml\")    #通过beautifulsoup中lxml解码器解析源码\n",
        "  #jake=soup.find_all('a',target='_blank'，src='images') #找a标签中，同时有target与src属性的\n",
        "  jakes=soup.find_all('img',onerror=\"javascript:this.src='/images/defaultTrack.png';\") #通过find_all找到soup中所有img标签且这个标签中含有onerror属性\n",
        "  html_list=[]                   #设置一个数组或者说集合来装符合要求的二级连接\n",
        "  #idx=0\n",
        "  for jake in jakes:              #打印含有img的父标签集合\n",
        "    if jake.parent !=None:          #加判断，把所有img中有父节点的匹配出来\n",
        "        html_list.append(jake.parent)    #通过加.append把收集好的数据存到html_list这个集合中\n",
        "        #idx=idx+1\n",
        "        #print(idx)检测是否逐一筛选出img页面\n",
        "  lst2 = list(set(html_list))          #去重，如果html_list中有重复的连接，这一步就可以去掉\n",
        "  #print(lst2)检测是否去重成功\n",
        "  #all_music = lst2.find_all('a', {'href': re.compile('/track/.*2$')}) #找list2中符合条件的href正则法\n",
        "  \n",
        "  pages = []                  #设置pages集合来装完整的一级目录集合，因为上一步搜集到的是相对地址需要通过这一个\n",
        "  for link in lst2:\n",
        "    current_url = \"http://www.2bulu.com/\"  #如果是相对地址就拼接\n",
        "    relative_url=link['href']         #找到单个link中href的属性内容\n",
        "    #print(relative_url)           #确认是否找到\n",
        "    complete_url = urljoin(current_url, relative_url)  #urljoin接口实现拼接，即把第1个参数与第2个参数拼接在一起\n",
        "    pages.append(complete_url)        #把拼接好的完整链接用append累加存在pages这个集合内\n",
        "  return pages                \n",
        "#（4）解析二级目录源代码，并提取关键词和标签\n",
        "def parse_page(text):\n",
        "  #url=\"http://www.2bulu.com/track/t-ockmzMSsUKzp%252FR2KBg5Tzw%253D%253D.htm?tabType=2xia\"  #可以使用这个函数实现单独测试网页是否需要动态加载\n",
        "  #print(page_sourse)\n",
        "  soup = BeautifulSoup(text, 'lxml')    #使用BeautifulSoup解析text传过来的网站，text是形参，第2个参数lxml是BeautifulSoup3个解析工具之一，它的速度最快\n",
        "  #print(soup)查看二级页面是否也是动态加载，如果是则需要改用chrome\n",
        "  lis = soup.find(\"ul\", id =\"track_marker_div\",class_=\"biaozhu_list\")  #查找ul标签中，属性id=track_marker_div,属性class=biaozhu_list的整个ul\n",
        "  #print(lis)                 #打印下确认是否正确抓取\n",
        "  datas=[]                  #定义datas的数组装二级目录中想要抓取的标签数据\n",
        "  for li in lis:              #循环找lis中即ul标签中的每一个li子标签\n",
        "    time.sleep(1)              #找到一个下载完毕后就睡1s\n",
        "    #print(li)               #确认是否成功找到\n",
        "    times=li.find('p',class_='date').text  #找时间标签，find是只找第一个p标签，属性为date的,后面的.text就是提取此标签中的文本内容\n",
        "    if re.findall(r'\\d{1,4}-\\d{1,2}-\\d{1,2} \\d{1,2}:\\d{1,2}', times):         #当找到这样的结构2019-12-28 08:02就进行下面的切割{1，4}对应每个位置有多少数字 \n",
        "      result_year = time.strftime(\"%Y\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\")) #取年份，当年月日时都匹配正确后，改变输出，这是只输出年份\n",
        "      result_month = time.strftime(\"%m\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\")) #取月份   \n",
        "      result_day = time.strftime(\"%d\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\")) #strptime作用是将第1个参数的格式与第2个参数匹配\n",
        "      result_hour = time.strftime(\"%H\", time.strptime(times, \"%Y-%m-%d %H:%M:%S\"))#strftime作用将第2个参数输出成第1个参数的形式\n",
        "    #names=li.find('p',class_='text').text\n",
        "    name=li.find('p',{\"class\": re.compile('[^\\d,\\.]+')}).string #通过find提取p标签中，过滤数字和点.的文本标注内容，^\\d的意思是除开数字，\\.就是除了.因为是特殊符号所有要加反斜杠 \n",
        "    names=re.findall('[^\\s,\\d,\\.:,\\-]+',name)          #实现匹配除空格、数字、标点(.)的字符\n",
        "    link=li.find('img',class_='lazy_img')            #这个标签内存了图片的下载链接\n",
        "    if (link is not None):                  #加个判断，防止没找到的时候报错，只有在找到才进行下面的提取下载图片工作\n",
        "      links = link[\"data-original\"]             #用links来装下载链接\n",
        "      id=li.find('div',class_='point').get('id')        #查找每张图片的id，通过get可以得到id=\"225449118\"它属性的值，因为不是文本所以用get\n",
        "      id=int(id)                        #id是属性值，所以要强转为int型 \n",
        "      datas.clear()                      #append会累加每次的数据，所以用clear清空datas中的数据，这样datas中每次只有新图片的所有信息\n",
        "      datas.append({\"n\":names,\"年\":result_year,\"月\":result_month,\"日\":result_day,\"时\":result_hour,\"id\":id}) #数据写进datas中\n",
        "    #下载照片\n",
        "      response = requests.get(links)             #通过requests.get实现下载照片\n",
        "      fb = open('/content/drive/My Drive/骑田岭/图片/%s.jpg'%str(id),'wb')  #创建文件以wb代表以二进制方式写入，图片一般都是用二进制编码后写入，%s对应的是后面的%str(id)即以id名命名图片\n",
        "      fb.write(response.content)                #写入request访问链接得到的内容\n",
        "      fb.close()                        #关闭，减少内存占用\n",
        "    #下载标签数据\n",
        "      with open(\"/content/drive/My Drive/骑田岭/json/%s.json\"%str(id), \"w\") as fout: #创建文件把标签属性信息写入并以json格式保存\n",
        "        for data in datas:                              #提取datas中的每个标签，其实如果有上面的clear一步就可以不这样写，多余了\n",
        "          fout.write(json.dumps(data, ensure_ascii=False)+\"\\n\")           #json.dumps实现写入\n",
        "    else:                            #如果没有找到图片链接就跳过\n",
        "      pass\n",
        "    #以src命名图片和json文件\n",
        "  return datas \n",
        "\n",
        "for idx in range(0,23):                     #for循环，设置idx参数，range实现是闭区间，即range(0,24)，则idx从0开始24结束，idx=0...\n",
        "  #result_list = []\n",
        "  time.sleep(1)                          #二级页面抓取完后睡1秒\n",
        "  #九嶷山http://www.2bulu.com/track/list-%E4%B9%9D%E5%B6%B7%E5%B1%B1-----23.htm?sortType=2\n",
        "  #骑田岭http://www.2bulu.com/track/list-%E9%AA%91%E7%94%B0%E5%B2%AD-----1.htm?sortType=2\n",
        "  url = f\"http://www.2bulu.com/track/list-%E9%AA%91%E7%94%B0%E5%B2%AD-----{idx+1}.htm?sortType=2\" #通过f,{idx+1},实现翻页抓取\n",
        "  print(\"spider当前正在爬取第 %s 页的轨迹\"%(idx+1))\n",
        "  text = start_requests(url)                  #调用start_requests(url)打印正在爬取的链接\n",
        "  time.sleep(1)\n",
        "  pageurls = parse_music(text)                 #清理好的只有图片的二级链接集合\n",
        "  print(\"当前页面含有图片标记的轨迹数为 %s 条\"%len(pageurls))\n",
        "  flag=1                             #设置个参数标记正在打印的链接\n",
        "  for pageurl in pageurls:                  #提取含有二级链接集合中的每一个链接\n",
        "    print(\"正在爬取第 %s 条链接 开始时间： %s\"%(flag,time.ctime())) #打印时间\n",
        "    flag=flag+1\n",
        "    page = start_requests(pageurl)             #打印正在分析的链接\n",
        "    jake = get_page_sourse(pageurl)            #用chrome模拟打开二级页面得到网页源码并存入jake\n",
        "    parse_page(jake)                     #分析源码实现下载保存\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqVEBJFOt8Bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#查找有效代理，防止封IP\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import OrderedDict\n",
        "from threading import Thread\n",
        "from datetime import datetime\n",
        "start_time = datetime.now()\n",
        "\n",
        "def get_xici_proxy(url,headers):\n",
        "    response = requests.get(url, headers=headers).content\n",
        "    res = response.decode('utf-8')\n",
        "    soup = BeautifulSoup(res, 'lxml')\n",
        "    tag_tr_all = soup.find_all('tr')\n",
        "\n",
        "    info_names = tag_tr_all[0].get_text().strip().split('\\n')\n",
        "    # global proxy_list\n",
        "    t_list = []\n",
        "    for tag_tr in tag_tr_all[1:]:  \n",
        "        tag_td = tag_tr.find_all('td')#\n",
        "        try:\n",
        "            country = tag_td[0].img['alt'] \n",
        "        except TypeError:#\n",
        "            country = 'None'\n",
        "        try:\n",
        "            ip_info_list = [td.get_text(strip=True) for td in tag_td[1:]] \n",
        "            ip_info_list.insert(0,country)\n",
        "            ip_info_dict = OrderedDict(zip(info_names,ip_info_list))\n",
        "            t = Thread(target =check_proxy,args=(ip_info_dict,))\n",
        "            t_list.append(t)\n",
        "        except Exception as e:\n",
        "            print(\"登录错误\",e)\n",
        "    for i in range(len(tag_tr_all[1:])):\n",
        "        t_list[i].start()\n",
        "    for i in range(len(tag_tr_all[1:])):\n",
        "        t_list[i].join()\n",
        "\n",
        "def check_proxy(info):\n",
        "    proxy = {info['类型'].lower():r\"{}://{}:{}\".format(info['类型'].lower(),info['IP地址'],info['端口']),}\n",
        "    try:\n",
        "        response1 = requests.get(r\"http://httpbin.org/get\",proxies=proxy,timeout=10)\n",
        "        #print(response1.status_code)\n",
        "        if response1.status_code==200:\n",
        "            info['proxy'] = proxy\n",
        "            proxy_list.append(info)\n",
        "    except Exception as e:\n",
        "        pass#\n",
        "if __name__ == \"__main__\":\n",
        "    url = r\"https://www.xicidaili.com/nn\"\n",
        "    headers = {\n",
        "        'User-Agent': \"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\"\n",
        "    }\n",
        "    proxy_list = [] \n",
        "    get_xici_proxy(url,headers)\n",
        "\n",
        "    print(\"有效代理数：\",len(proxy_list))\n",
        "    print(\"第二个代理地址：\",proxy_list)\n",
        "    print(\"第二个代理地址类型：\",proxy_list[1].get('类型'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTBrX8xyvMx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from selenium import webdriver\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--proxy-server=http://222.95.240.228:3000') #这里填入修改的IP地址\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('User-Agent:\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\"')\n",
        "driver = webdriver.Chrome('chromedriver', options=options)\n",
        "driver.get(\"http://httpbin.org/ip\")\n",
        "page_sourse=driver.page_source\n",
        "print(page_sourse)\n",
        "driver.close()\n",
        "\n",
        "# 查看本机ip，查看代理是否起作用\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS_8zq99b6Yu",
        "colab_type": "code",
        "outputId": "177fe9ff-5270-4bb0-e4f5-3311819bbf57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#删除重复的图片和对应的xml文件\n",
        "# -*- coding: cp936 -*-\n",
        "import hashlib\n",
        "import os\n",
        "import time\n",
        "\n",
        "#排序filelist = sorted(filelist)\n",
        "def getmd5(filename):\n",
        "    \"\"\"\n",
        "    获取文件 md5 码\n",
        "    :param filename: 文件路径\n",
        "    :return: 文件 md5 码\n",
        "    \"\"\"\n",
        "    file_txt = open(filename, 'rb').read()\n",
        "    # 调用一个md5对象\n",
        "    m = hashlib.md5(file_txt)\n",
        "    # hexdigest()方法来获取摘要（加密结果）\n",
        "    return m.hexdigest()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 文件夹路径\n",
        "    path = input(\"path: \")\n",
        "    # 键为文件大小, 值为列表（文件路径、md5）\n",
        "    all_size = {}\n",
        "    total_file = 0\n",
        "    total_delete = 0\n",
        "    # 开始时间\n",
        "    start = time.time()\n",
        "    # 遍历文件夹下的所有文件\n",
        "    for file in os.listdir(path):\n",
        "        # 文件数量加 1\n",
        "        total_file += 1\n",
        "        # 文件的路径\n",
        "        real_path = os.path.join(path, file)\n",
        "        # 判断文件是否是文件\n",
        "        if os.path.isfile(real_path) == True:\n",
        "            # 获取文件大小\n",
        "            size = os.stat(real_path).st_size\n",
        "            # md5(默认为空)\n",
        "            size_and_md5 = [\"\"]\n",
        "            # 如果文件大小已存在\n",
        "            if size in all_size.keys():\n",
        "                # 获取文件的md5码\n",
        "                new_md5 = getmd5(real_path)\n",
        "                # 大小相同，md5 为空，添加md5\n",
        "                if all_size[size][0] == \"\":\n",
        "                    all_size[size][0] = new_md5\n",
        "                # md5 已存在，删除\n",
        "                if new_md5 in all_size[size]:\n",
        "                    print('删除的图片路径： ', real_path)\n",
        "                    pure_path=os.path.splitext(os.path.basename(real_path))[0]   #实现把绝对路径都去掉，只剩下最后的文件名，[0]代表split切割后的第一个参数位置\n",
        "                    pure= '/content/drive/My Drive/骑田岭/xml/%s'%pure_path + '.xml' #如果只删除照片，则把52-54三行删掉就可以了\n",
        "                    os.remove(pure)\n",
        "                    #print(\"删除的xml路径为： \"，pure)\n",
        "                    os.remove(real_path)\n",
        "                    total_delete += 1\n",
        "                else:\n",
        "                    # md5 不存在，进行添加\n",
        "                    all_size[size].append(new_md5)\n",
        "            else:\n",
        "                # 如果文件大小不存在，则将此文件大小添加到 all_size 字典中\n",
        "                all_size[size] = size_and_md5\n",
        "    # 结束时间\n",
        "    end = time.time()\n",
        "    time_last = end - start\n",
        "    print('文件总数：', total_file)\n",
        "    print('删除个数：', total_delete)\n",
        "    print('耗时：', time_last, '秒')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "path: /content/drive/My Drive/骑田岭/图片\n",
            "删除的图片路径：  /content/drive/My Drive/骑田岭/图片/105763120.jpg\n",
            "删除的图片路径：  /content/drive/My Drive/骑田岭/图片/105766377.jpg\n",
            "删除的图片路径：  /content/drive/My Drive/骑田岭/图片/105766379.jpg\n",
            "删除的图片路径：  /content/drive/My Drive/骑田岭/图片/105766380.jpg\n",
            "删除的图片路径：  /content/drive/My Drive/骑田岭/图片/105766381.jpg\n",
            "删除的图片路径：  /content/drive/My Drive/骑田岭/图片/105766384.jpg\n",
            "删除的图片路径：  /content/drive/My Drive/骑田岭/图片/105766386.jpg\n",
            "删除的图片路径：  /content/drive/My Drive/骑田岭/图片/105766382.jpg\n",
            "文件总数： 280\n",
            "删除个数： 8\n",
            "耗时： 0.06737899780273438 秒\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vOWBlCAfg7K",
        "colab_type": "code",
        "outputId": "7888b280-63b9-4211-ad9c-bd30ca62815a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#统计文件夹中的文件数\n",
        "import os\n",
        "path =\"/content/drive/My Drive/九嶷山/随机抽取对应的图片\"\n",
        "#（骑田岭）   图片280，去重后272\n",
        "#（萌渚岭）九嶷山图片1410，去重后692\n",
        "#（越城岭）舜皇山图片1209，去重后904\n",
        "#（都庞岭）韭菜岭图片12923,去重后5988\n",
        "count = 0\n",
        "for fn in os.listdir(path): #fn 表示的是文件名#原来共12923个图片\n",
        "        count = count+1\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcRuVgRui_ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/drive/My\\ Drive/骑田岭/new骑田岭/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQlaQiRhjjy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/My\\ Drive/骑田岭/xml/160097868.xml /content/drive/My\\ Drive/骑田岭/new骑田岭/new1骑田岭/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYGDdvZujybQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/骑田岭/new1骑田岭 /content/drive/My\\ Drive/骑田岭/new骑田岭"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvA55FBbSdV9",
        "colab_type": "code",
        "outputId": "7c231145-0c66-4a92-8d70-4ed826013db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#把所有xml汇总成一个csv文件\n",
        "'''\n",
        "只需修改三处，第一第二处改成对应的文件夹目录，\n",
        "第三处改成对应的文件名，这里是train.csv\n",
        "os.chdir('D:\\\\python3\\\\models-master\\\\research\\\\object_detection\\\\images\\\\train')\n",
        "path = 'D:\\\\python3\\\\models-master\\\\research\\\\object_detection\\\\images\\\\train'\n",
        "xml_df.to_csv('train.csv', index=None)\n",
        "'''\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        " \n",
        "os.chdir('/content/drive/My Drive/都庞岭/xml')\n",
        "path = '/content/drive/My Drive/都庞岭/xml'\n",
        " \n",
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        value = (root.find('id').text,root.find('年').text,root.find('月').text,root.find('日').text,root.find('时').text)\n",
        "        xml_list.append(value)\n",
        "    column_name = ['id', '年', '月', '日', '时']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df\n",
        " \n",
        " \n",
        "def main():\n",
        "    image_path = path\n",
        "    xml_df = xml_to_csv(image_path)\n",
        "    xml_df.to_csv('/content/drive/My Drive/机器学习分析的总数据/都庞岭属性.csv', index=None)\n",
        "    print('Successfully converted xml to csv.')\n",
        " \n",
        " \n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JftupsUBJOlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#随机抽取\n",
        "##深度学习过程中，需要制作训练集和验证集、测试集。\n",
        "\n",
        "import os, random, shutil\n",
        "def moveFile(fileDir):\n",
        "        pathDir = os.listdir(fileDir)    #取图片的原始路径\n",
        "        filenumber=len(pathDir)\n",
        "        rate=0.111    #自定义抽取图片的比例，比方说100张抽10张，那就是0.1\n",
        "        picknumber=int(filenumber*rate) #按照rate比例从文件夹中取一定数量图片\n",
        "        sample = random.sample(pathDir, picknumber)  #随机选取picknumber数量的样本图片\n",
        "        print (sample)\n",
        "        for name in sample:\n",
        "                shutil.copyfile(fileDir+name, tarDir+name)#复制到新文件夹\n",
        "                #移动到新文件夹shutil.move(fileDir+name, tarDir+name)\n",
        "        return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tfileDir = \"/content/drive/My Drive/骑田岭/图片/\"    #源图片文件夹路径\n",
        "\ttarDir = '/content/drive/My Drive/骑田岭/new骑田岭/'    #移动到新的文件夹路径\n",
        "\tmoveFile(fileDir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9Sz-KD7SOX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/drive/My\\ Drive/骑田岭/new骑田岭/new1骑田岭/1.xml /content/drive/My\\ Drive/骑田岭/new骑田岭/new1骑田岭/xml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAliW3U_RxWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#通过图片找到对应的xml文件\n",
        "#coding=utf-8\n",
        "import os\n",
        "#import os.path\n",
        "import shutil  #Python文件复制相应模块\n",
        " \n",
        "label_dir=r'/content/drive/My Drive/都庞岭/xml'  #所有xml文件所在文件夹\n",
        "annotion_dir='/content/drive/My Drive/都庞岭/随机抽取对应的xml'  #粘贴对应图片名称的xml文件到指定文件夹\n",
        "path = '/content/drive/My Drive/都庞岭/随机抽取对应的图片'   #图片文件夹\n",
        "path_list = os.listdir(path)# os.listdir(file)会历遍文件夹内的文件并返回一个列表\n",
        "# print(path_list)\n",
        "path_name=[]  # 定义一个空列表,不需要path_list中的后缀名\n",
        "# 利用循环历遍path_list列表并且利用split去掉后缀名\n",
        "for i in path_list:\n",
        "    path_name.append(i.split(\".\")[0])\n",
        "# print(path_name)\n",
        "# 排序一下\n",
        "path_name.sort()\n",
        "for file_name in path_name:\n",
        "    # \"a\"表示以不覆盖的形式写入到文件中,当前文件夹如果没有\"save.txt\"会自动创建\n",
        "    with open(\"/content/drive/My Drive/save.txt\",\"a\") as f:\n",
        "        f.write(file_name + \"\\n\")\n",
        "        #print(file_name)\n",
        "    f.close()\n",
        "f = open(\"/content/drive/My Drive/save.txt\",\"r\")   #设置文件对象\n",
        "lines= f.readlines() \n",
        "print (lines)\n",
        "s=[]\n",
        "i=0\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    # print (line)  \n",
        "    tempxmlname='%s.xml'%line\n",
        "    # print(tempxmlname)\n",
        "    xmlname=os.path.join(label_dir,tempxmlname)\n",
        "    # print (xmlname)\n",
        "    os.listdir(label_dir)\n",
        "    shutil.copy(xmlname,annotion_dir)\n",
        "    i+=1\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMfgn7dOT1bC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo rmdir /content/drive/My\\ Drive/骑田岭/new骑田岭/对应xml/.ipynb_checkpoints "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akb5nZYNR5Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /content/drive/My\\ Drive/骑田岭/new骑田岭/new1骑田岭/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxJIrX5Et2YE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from os.path import join, getsize\n",
        "src_dir='/content/drive/My Drive/都庞岭/随机抽取对应的图片'\n",
        "def zip_file(src_dir):\n",
        "    zip_name = src_dir +'.zip'\n",
        "    z = zipfile.ZipFile(zip_name,'w',zipfile.ZIP_DEFLATED)\n",
        "    for dirpath, dirnames, filenames in os.walk(src_dir):\n",
        "        fpath = dirpath.replace(src_dir,'')\n",
        "        fpath = fpath and fpath + os.sep or ''\n",
        "        for filename in filenames:\n",
        "            z.write(os.path.join(dirpath, filename),fpath+filename)\n",
        "            print ('==压缩成功==')\n",
        "    z.close()\n",
        "zip_file(src_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ59maBQHipH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#csv转xlsx\n",
        "import pandas as pd\n",
        "\n",
        "def csv_to_xlsx_pd():\n",
        "    csv = pd.read_csv('/content/drive/My Drive/九嶷山/九嶷山.csv', encoding='utf-8')\n",
        "    csv.to_excel('/content/drive/My Drive/九嶷山/九嶷山.xlsx', sheet_name='data')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    csv_to_xlsx_pd()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}